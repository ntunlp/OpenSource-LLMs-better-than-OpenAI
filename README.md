# OpenSource-LLMs-better-than-ChatGPT

### Datasets

#### Long-input capacity

1. [ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding](https://arxiv.org/abs/2305.14196). Uri Shaham et al, EMNLP 2023. Long-input benchmark containing 10 datasets: 2 summarization tasks (GovReport and SummScreenFD), 2 query-based summarization tasks (QMSum and SQuALITY), 4 question-answering datasets (Qasper, NarrativeQA, QuALITY, MuSiQue), 2 aggregation tasks (SpaceDigest, BookSumSort). 

#### Code

1. [Evaluating large models trained on code (**HumanEval** benchmark)](https://arxiv.org/abs/2107.03374). Mark Chen et al, 2021. 164 hand-written programming problems. Each problem includes a function signature, docstring, body, and several unit tests, with an average of 7.7 tests per problem.

#### Math

1. [Training verifiers to solve math word problems (**GSM8K** benchmark)](https://arxiv.org/abs/2110.14168). Karl Cobbe et al, 2021. 8.5K (7.5k training + 1k test) high quality grade school math problems created by human problem writers. Each problem takes between 2 and 8 steps to solve, and solutions primarily involve performing a sequence of elementary calculations using basic arithmetic operations.

### High-performing open-source LLMs

#### Better training techniques

#### Better fine-tuning techniques

#### Better inference techniques 
